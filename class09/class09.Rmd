---
title: "class09.Rmd"
author: "Christopher Cheng"
date: "2/4/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## K-means clustering

The main k-means function in R is called `kmeans()`. Let's play with it here. 

Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results

```{r}
# Generate some example data for clustering
# rnorm generates two two-column datasets with x/y coordinates centered around -3 and 3 respectively. This is then combined into a matrix (4x4) and stored in tmp
tmp <- c(rnorm(30, -3), rnorm(30,3))

# setting the x coordinates as (-,+) and y coordinates as (+,-) gives us the desired plot
x <- cbind(x = tmp, y = rev(tmp))
plot (x)


# Generate k-means clustering with example dataset
km <- kmeans(x, centers = 2, nstart = 20)
km
```

Q. How many points are in each cluster?
Answ: 30 each

Q. What ‘component’ of your result object details
- cluster size? Answ: km$size
- cluster assignment/membership? Answ: km$cluster
- cluster center? Answ: km$centers

```{r}
length(km$cluster)
table(km$cluster)
```


Plot x colored by the kmeans cluster assignment and
add cluster centers as blue points

```{r}
plot(x, col = km$cluster + 40)
points(km$centers, col = "blue", pch = 2, cex = 1.4)

```

## Hierarchical clustering

The main Hierarchical clustering function in R is called `hclust()` 

An important point here is you have to calculate the distance matrix from your input data before calling `hclust()`


```{r}
# First we need to calculate point (dis)similarity as the Euclidean distance between observations

dist_matrix <- dist(x)

#The hclust() function returns a hierarchical clustering model

hc <- hclust(d = dist_matrix)

# the print method is not so useful here
hc
```



Let's take a closer look
```{r}
dist_matrix <- dist(x)

dim(dist_matrix)

View(as.matrix(dist_matrix))

dim(x)

dim(as.matrix(dist_matrix))

# Note: Symmetrical pairwise distance matrix 
```


Folks often view the results of Hierarchical clustering graphically. Let's try passing this to the `plot()` function.

```{r}
# Create hierarchical cluster model: hc
hc <- hclust(dist(x))

# We can plot the results as a dendrogram
plot(hc)

# What do I notice? Does the dendrogram make sense based on my knowledge of x?

```

To get a cluster membership vector I need to "cut" the tree at a certain height to yield my sepearate cluster branches

```{r}
# Draws a dendrogram
plot(hc)
abline(h = 6, col = "red", lty = 2)
abline(h = 4, col = "blue")
gp4 <- cutree(hc, h = 4)
cutree(hc, h = 6) # Assign a cut at height = 6
cutree(hc, k = 2) # Cut into k # of groups
```



# Hierarchical clustering: My turn
```{r}
# Step 1. Generate some example data for clustering

x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), 
  matrix(c(rnorm(50, mean = 1, sd = 0.3), 
  rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
  
colnames(x) <- c("x", "y")

# Step 2. Plot the data without clustering
plot(x)

# Step 3. Generate colors for known clusters (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )

plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree()
functions to return 2 and 3 clusters
```{r}
hc <- hclust(dist(x))
plot(hc)
abline(h = 1.65, col = "red")

# To get cluster membership vector, we use `cutree()` and then use `table()` to tabulate up how many members in each cluster we have. 

grps <- cutree(hc, k = 3)
table(grps)
```

Q. How does this compare to your known 'col' groups?
Answ: There's weird overlap between groups. Program can't decide what goes where. Use col=grps to distinguish between these.

```{r}
plot(x, col = grps)
```

## Principal Component Analysis (PCA): Dimensionality reduction, visualization, and 'structure' analysis

# Working with UK Food
```{r}
x <- read.csv("UK_foods.csv", row.names = 1)
dim(x)
nrow(x)
ncol(x)


#View the components individually or in a separate tab
head(x)
tail(x)
View(x)
```


## Spotting major differences and trends
# Lets make some plots to explore our data a bit more

```{r}
# change beside = True to get opposite orientation 

barplot(as.matrix(x), beside = F, col = rainbow(nrow(x)))
```

Using pairs plot
```{r}
pairs(x, col = rainbow(10), pch = 16)
```


## Principal Component Analysis (PCA) with the  `prcomp()` function

```{r}
#Use the prcomp() PCA function

pca <- prcomp(t(x))

# looking into the components of our function output(pca). What is in my result object 'pca'?

attributes(pca)

# Providing a summary of the PCA
summary(pca)
```

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab = "PC2", xlim = c(-270,500))

# Adding text labels over our data points
text(pca$x[,1], pca$x[,2], colnames(x), col =c("black", "red", "blue", "darkgreen"))
```


To see how much variation lies within our PCA
```{r}
v <- round(pca$sdev^2/sum(pca$sdev^2) * 100)
v

# or the second row here...
z <- summary(pca)
z$importance

# This information can also be summarized graphically
barplot(v, xlab = "Principal Component", ylab= "Percent Variation")
```


Digging Deeper (variable loadings)
```{r}
#Just focusing on PC1 as it accounts for >90% of the variance
par(mar = c(10,3,0.35,0))
barplot(pca$rotation[,1], las = 2)
```


Biplots
```{r}
# The inbuilt biplot() can be useful for small datasets
biplot(pca)
```


## PCA of RNA-seq data

```{r}
rna.data <- read.csv("expression.csv", row.names =1)
head(rna.data)
```

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un ploished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2])
```


```{r}
## Variance captured per PC 
pca.var <- pca$sdev^2

## Precent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```


```{r}
barplot(pca.var.per, main="Scree Plot", 
        xlab="Principal Component", ylab="Percent Variation")
```


```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```


```{r}
## Another way to color by sample type
## Extract the first 2 characters of the sample name
sample.type <- substr(colnames(rna.data),1,2)
sample.type
```


```{r}
## now use this as a factor input to color our plot
plot(pca$x[,1], pca$x[,2], col=as.factor(sample.type), pch=16)
```

```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes 
```

```{r}
sessionInfo()
```





































































