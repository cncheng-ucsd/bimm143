---
title: "class10.Rmd"
author: "Christopher Cheng"
date: "2/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploratory data analysis

# Preparing the data

```{r}
# Read in the csv data file
wisc.df <- read.csv("WisconsinCancer.csv")

# View the initial data frame values 
head(wisc.df)

# Convert the features of the data: wisc.data (convert columns to a matrix)
wisc.data <- as.matrix(wisc.df[,3:32])

# There are some funky things in this dataset that we will ignore for our analysis. This includes the first and second ID and Diagnosis columns and the funny last X column (zip codes-redacted, col33)

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id
#head(wisc.data) #check to see if the rownames propagated properly

# Create a diagnosis vector for later
diagnosis <- vector()

# Answering Q1-3
#Q1
dim(wisc.data)
nrow(wisc.data)

#Q2
table(wisc.df$diagnosis)

#Q3
colnames(wisc.data)
length(grep("_mean", colnames(wisc.data)))
```
Hint: Use dim, nrow, table, length, and grep 

Q1. How many observations are in this dataset? A: 569

Q2. How many of the observations have a malignant diagnosis? A: 212

Q3. How many variables/features in the data are suffixed with _mean? A: 10



## Principal Component Analysis

# Performing PCA

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

round(apply(wisc.data, 2, sd), 2)
```


Before we turn to PCA we need to think, or consider, whether we should SCALE our input.

It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:

The input variables use different units of measurement.

- The input variables have significantly different variances.
- Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the `colMeans()` and `apply()` functions like you’ve done before.

```{r}
# Perform PCA on wisc.data 
wisc.pr <- prcomp(wisc.data, scale. = TRUE)
```


```{r}
# Look at a summary of the results
summary(wisc.pr)
```
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)? A: 0.4427

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data? A: 3

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data? A: 7


# Interpreting PCA Results

```{r}
# Create a biplot 
biplot(wisc.pr)
```

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why? A: This plot is a mess. IDs have no indication of what they're related to.Rownames are used as the plotting character for biplots like this one which can make trends rather hard to see. In fact, this plot is very poor. 

```{r}
# Use attributes to tell us what's in wisc.pr
attributes(wisc.pr)
```

```{r}
# Plot components 1 and 2 of wisc.pr
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = wisc.df$diagnosis, xlab = "PC1", ylab = "PC2")
abline(h = 0, col = "gray", lty = 2)
abline(v = 0, col = "gray", lty = 2)
```

```{r}
# Generate a scatter plot observations by components 1 and 2
plot(wisc.pr$x, col = wisc.df$diagnosis , xlab = "PC1", ylab = "PC2")
```


Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? A: Plot 1 has a lot cleaner cut separating the two subgroups due to accounting for more variance in the original data

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = wisc.df$diagnosis, xlab = "PC1", ylab = "PC3")
```

# Variance Explained
```{r}
# Calculate the variance of each component
pr.var <-wisc.pr$sdev ^ 2
head(pr.var)
```


```{r}
# Variance explained by each principal component: pve
pve <-  pr.var / wisc.pr$sdev
  
# Plot variance explained for each principle 
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0,1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Percent of Variance Explained",
        names.arg = paste0("PC", 1:length(pve)), las = 2, axes = FALSE)
axis(2, at = pve, labels = round(pve,2) * 100)
```

```{r}
# ggplot based graph
# install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```


## Hierarchical clustering

# Hierarchical clustering of case data
```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
```
```{r}
# Calculate the Eculidean distances between all pairs of observations in the new scaled dataset and assign the result
data.dist <- dist(data.scaled)
```

```{r}
# Create a hierarchical clustering model
wisc.hclust <- hclust(data.dist, method = "complete")
```

Results of hierarchical clustering
Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(wisc.hclust, col = "red", lty = 2, h = 19.5)
```

Selecting number of clusters
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

We can use `table()` to compare the cluster membership to actual diagnoses
```{r}
table(wisc.hclust.clusters, wisc.df$diagnosis)
```

## K-means clustering
# K means clustering and comparing results

```{r}
wisc.km <- kmeans(scale(wisc.data), centers = 2, nstart = 20)

table(wisc.km$cluster, wisc.df$diagnosis)

table(wisc.km$cluster, wisc.hclust.clusters)
```




## Combining Methods
# Clustering on PCA results

First, let's see if we can cluster the original dat
```{r}
wisc.hc <- hclust(dist(wisc.data))
plot(wisc.hc)
```

This does not look good! Let's try and combine the results of PCA with clustering...


Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:3]), method = "ward.D2")
plot(wisc.pr.hclust)
```


```{r}
grps <- cutree(wisc.pr.hclust, k = 2)
table(grps)
```


```{r}
# Visually look at that
plot(wisc.pr$x[,1:2], col = grps)
```


To get our clusters out of this tree, we need to CUT it with the `cutree()` function.
```{r}
grps3 <- cutree(wisc.pr.hclust, k = 2)
table(grps3)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = grps3)
```

We can use the `table()` function to compare the $diagnosis vector with our cluster results vector.
```{r}
table(grps3, wisc.df$diagnosis)
```

```{r}
# Turn our groups into a factor and reorder the level so cluster 2 comes first
g <- as.factor(grps)
levels(g)

g <- relevel(g,2)
levels(g)
```
```{r}
# Plot using our re-ordered factor
plot(wisc.pr$x[,1:2], col = g)
```

```{r}
#install.packages("rgl")
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
rglwidget(width = 400, height = 400)
```


## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
```{r}

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

Q14. How well does the newly created model with four clusters separate out the two diagnoses?


Q15. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

Skip Section 6

## Prediction
We will use the `predict()` function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```


Barry's plots:
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = wisc.df$diagnosis)
points(npc[,1], npc[,2], col = "blue", pch = 15, cex = 3 )
text(npc[,1], npc[,2], labels = c(1,2), col = "white")
```

```{r}
plot(wisc.pr$x[,1:2], col = g)
points(npc[,1], npc[,2], col = "blue", pch = 16, cex = 3)
text(npc[,1], npc[,2], c(1,2), col = "white")
```

Q17. Which of these new patients should we prioritize for follow up based on your results?
Answer: Patient 2 



```{r}
sessionInfo()
```


























































































































































































